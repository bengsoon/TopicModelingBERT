{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Lord of The Rings trilogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/01_fellowship.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path: str) -> str:\n",
    "    ''' Reads utf-8 text file '''\n",
    "    with open(Path(file_path), \"r\", encoding=\"utf-8\") as f_out:\n",
    "        data = f_out.read()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the first 2000 characters of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_Chapter 1_\\n            A Long-expected Party\\n\\n     When Mr. Bilbo Baggins of Bag End announced that he would shortly be celebrating his eleventy-first birthday with a party of special magnificence, there was much talk and excitement in Hobbiton.\\n     Bilbo was very rich and very peculiar, and had been the wonder of the Shire for sixty years, ever since his remarkable disappearance and unexpected return. The riches he had brought back from his travels had now become a local legend, and it was popularly believed, whatever the old folk might say, that the Hill at Bag End was full of tunnels stuffed with treasure. And if that was not enough for fame, there was also his prolonged vigour to marvel at. Time wore on, but it seemed to have little effect on Mr. Baggins. At ninety he was much the same as at fifty. At ninety-nine they began to call him _well_-preserved, but _unchanged_ would have been nearer the mark. There were some that shook their heads and thought this was too much of a good thing; it seemed unfair that anyone should possess (apparently) perpetual youth as well as (reputedly) inexhaustible wealth.\\n     'It will have to be paid for,' they said. 'It isn't natural, and trouble will come of it!'\\n     But so far trouble had not come; and as Mr. Baggins was generous with his money, most people were willing to forgive him his oddities and his good fortune. He remained on visiting terms with his relatives (except, of course, the Sackville-Bagginses), and he had many devoted admirers among the hobbits of poor and unimportant families. But he had no close friends, until some of his younger cousins began to grow up.\\n     The eldest of these, and Bilbo's favourite, was young Frodo Baggins. When Bilbo was ninety-nine, he adopted Frodo as his heir, and brought him to live at Bag End; and the hopes of the Sackville-Bagginses were finally dashed. Bilbo and Frodo happened to have the same birthday, September 22nd. 'You had better come and live here, Frodo my lad,' said Bil\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the book into list of chapters. Just by looking through the book, each chapter starts with \"\\_Chapter \\*\\_\". We will use regular expression, `regex` to create a filter: `\"\\_Chapter\\s\\d+\\_\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a regex filter for splitting chapters\n",
    "chapter_filter = \"\\_Chapter\\s\\d+\\_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book into a list of chapters \n",
    "chapters = re.split(chapter_filter, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we have correctly split the chapters by viewing the first 80 characters of each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of chapters extracted\n",
    "len(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 23 elements in the list of chapters, but we can see below that the Element 0 is empty spaces and the first chapter starts with Element 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0: \n",
      "Element 1: \n",
      "            A Long-expected Party\n",
      "\n",
      "    \n",
      "Element 2: \n",
      "            The Shadow of the Past\n",
      "\n",
      "   \n",
      "Element 3: \n",
      "            Three is Company\n",
      "\n",
      "     'You\n",
      "Element 4: \n",
      "            A Short Cut to Mushrooms\n",
      "\n",
      " \n",
      "Element 5: \n",
      "            A Conspiracy Unmasked\n",
      "\n",
      "    \n",
      "Element 6: \n",
      "            The Old Forest\n",
      "\n",
      "     Frodo \n",
      "Element 7: \n",
      "            In the House of Tom Bombadi\n",
      "Element 8: \n",
      "            Fog on the Barrow-Downs\n",
      "\n",
      "  \n",
      "Element 9: \n",
      "            At the Sign of\n",
      " The Prancin\n",
      "Element 10: \n",
      "            Strider\n",
      "\n",
      "     Frodo, Pippin\n",
      "Element 11: \n",
      "            A Knife in the Dark\n",
      "\n",
      "     A\n",
      "Element 12: \n",
      "            Flight to the Ford\n",
      "\n",
      "     Wh\n",
      "Element 13: \n",
      "            Many Meetings\n",
      "\n",
      "     Frodo w\n",
      "Element 14: \n",
      "            The Council of Elrond\n",
      "\n",
      "    \n",
      "Element 15: \n",
      "            The Ring Goes South\n",
      "\n",
      "     L\n",
      "Element 16: \n",
      "            A Journey in the Dark\n",
      "\n",
      "    \n",
      "Element 17: \n",
      "            The Bridge of Khazad-dûm\n",
      "\n",
      " \n",
      "Element 18: \n",
      "            Lothlórien\n",
      "\n",
      "     'Alas! I F\n",
      "Element 19: \n",
      "            The Mirror of Galadriel\n",
      "\n",
      "  \n",
      "Element 20: \n",
      "            Farewell to Lórien\n",
      "\n",
      "     Th\n",
      "Element 21: \n",
      "            The Great River\n",
      "\n",
      "     Frodo\n",
      "Element 22: \n",
      "            The Breaking of the Fellows\n"
     ]
    }
   ],
   "source": [
    "for no, chapter in enumerate(chapters):\n",
    "    print(f\"Element {no}:\", chapter[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the \"last\" chapter of The Fellowship of the Ring is \"The Breaking of the Fellowship\" (technically it's Chapter 10 of book II but there are 12 chapters in book I) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/huggingface_hub/file_download.py:560: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 468kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 82.9kB/s]\n",
      "Downloading: 100%|██████████| 10.1k/10.1k [00:00<00:00, 3.63MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 227kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 44.9kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 160kB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 192kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:13<00:00, 32.1MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 23.7kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 99.6kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 376kB/s]  \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 103kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 3.71MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 236kB/s]  \n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/torch/cuda/__init__.py:122: UserWarning: \n",
      "    Found GPU0 Quadro K5200 which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bengsoon/DataScienceProjects/TopicModelingBERT/TopicModeling_LOTR/TopicModeling_lotr.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bengsoon/DataScienceProjects/TopicModelingBERT/TopicModeling_LOTR/TopicModeling_lotr.ipynb#ch0000050vscode-remote?line=0'>1</a>\u001b[0m embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(chapters, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:164\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    161\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    163\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 164\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    167\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/transformers/models/mpnet/modeling_mpnet.py:550\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m     attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_shape, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 550\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[1;32m    552\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    553\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds)\n",
      "File \u001b[0;32m~/anaconda3/envs/TopicModelingBert/lib/python3.9/site-packages/transformers/modeling_utils.py:839\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    831\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape for input_ids (shape \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m) or attention_mask (shape \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    834\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[39m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    840\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(chapters, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('TopicModelingBert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac3096f7bd0d53f57dd42f12e7803a531fa96f4874d7bb49c79b5a732ca9f419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
